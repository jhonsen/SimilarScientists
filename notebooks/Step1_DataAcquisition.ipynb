{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] Prior to running this notebook, use this [tool](https://petscan.wmflabs.org/) to export a csv file (saved as _Scientists2levels.csv_). Choose Wikipedia titles under the **Category:Scientists**\n",
    "- [x] In this notebook, open the csv file and use the `title` column as input for extracting the actual articles w/ wikipedia-api\n",
    "- [x] Save as pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import wikipediaapi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract pages from WIKIPEDIA-API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open csv file\n",
    "scientistList = pd.read_csv('../data/Scientist2levels.csv')\n",
    "scientistList.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect names of scientists\n",
    "scientistNames = scientistList['title'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wikipedia wrapper function \n",
    "\n",
    "def getarticles(titles):\n",
    "    '''Function returns the titles of articles on wikipedia, in the form \n",
    "    of a list of dictionaries\n",
    "    input:\n",
    "        titles - is list of titles\n",
    "    '''\n",
    "    collection =[]\n",
    "    for each in titles:\n",
    "        wiki = wikipediaapi.Wikipedia(\n",
    "                language='en',\n",
    "                extract_format=wikipediaapi.ExtractFormat.WIKI\n",
    "        )\n",
    "        collection.append(wiki.page(each))\n",
    "        \n",
    "    return collection\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect articles under the category of scientist\n",
    "collection = getarticles(scientistNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total number of articles collected: ', len(collection))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unpack Results**\n",
    "- Collect titles and summaries from wiki articles, convert them into lists, \n",
    "- Then, create a pandas dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all the titles from dictionary (collection)\n",
    "# This step will take a while if you have a lot of data\n",
    "titles = [each.title for each in collection]\n",
    "summaries = [each.summary for each in collection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle these lists, just in case\n",
    "with open('../data/list_summaries.pkl','wb') as fout:\n",
    "    pickle.dump(summaries, fout)\n",
    "    \n",
    "# # Pickle the lists, just in case\n",
    "with open('../data/list_titles.pkl','wb') as fout:\n",
    "    pickle.dump(titles, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put these lists in to a dataframe\n",
    "df = pd.DataFrame(np.c_[titles, summaries], \n",
    "                  columns=['title', 'summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the dataframe for latter processing\n",
    "with open('../data/dfraw.pkl','wb') as fout:\n",
    "    pickle.dump(df, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next step**:\n",
    "- Clean the dataframe and do EDA, in Step2_Cleaning.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
